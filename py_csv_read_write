


#--------------read
df=spark.read.option("sep","|").csv(sPath)
df=spark.read.options(sep="|").csv(sPath)

#Read using schema
schema = StructType(
   [StructField('txn_dt', StringType(), True),
    StructField('merchant_id', StringType(), True),
    StructField('txn_count', IntegerType(), True),
    StructField('txn_amt', DecimalType(), True)
   ]
)
df_final=spark.read.option("sep","|").schema(schema).csv(sPath)

#-------------write
df.write.csv(PATH, header=True, emptyValue='')


Ex:

from datetime import datetime, date, timedelta
dt=datetime.now().strftime('%Y-%m-%d_%H_%M_%S')

dfData.coalesce(1).write.mode('overwrite').option('header', 'true').csv("/km/data/data_{}".format(dt), emptyValue='')

dfData.coalesce(1 if iRecordsCount/5000000<=0 else iRecordsCount/5000000).write.mode('overwrite').csv()

#to hive table
dfData.coalesce(1).write.insertInto("kmdb.mrch", overwrite=True )

#partitioned Hive table
from datetime import datetime, date, timedelta
dt_created = datetime.now().strftime('%Y-%m-%d_%H%M')
df_mrch_final = df_mrch.withColumn('extract_date', F.lit(dt_created))
df_mrch_final.coalesce(2).write.insertInto("kmdb.mrch_hist", overwrite=True )

#--------------option/options--------------------
.option("sep","|")
.option('header', 'true')
.option("compression", "gzip")  -->none/snappy/gzip

.options(sep='|', header='true', quoteAll='true')

#options syntax: 
#.option("sep"="|", "header"="true")


#-------------mode-------------
df.write.mode('append').options(sep='|', header='true', quoteAll='false').csv(output_path_matches, emptyValue='')

#append: adds data to existing directory
#overwrite: creates if the directory doesn't exist



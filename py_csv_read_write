


#--------------read
df=spark.read.option("sep","|").csv(sPath)
df=spark.read.options(sep="|").csv(sPath)

#Read using schema
schema = StructType(
   [StructField('txn_dt', StringType(), True),
    StructField('merchant_id', StringType(), True),
    StructField('txn_count', IntegerType(), True),
    StructField('txn_amt', DecimalType(), True)
   ]
)
df_final=spark.read.option("sep","|").schema(schema).csv(sPath)

#-------------write
df.write.csv(PATH, header=True, emptyValue='')


Ex:

from datetime import datetime, date, timedelta
dt=datetime.now().strftime('%Y-%m-%d_%H_%M_%S')

dfData.coalesce(1).write.mode('overwrite').option('header', 'true').csv("/km/data/data_{}".format(dt), emptyValue='')

dfData.coalesce(1 if iRecordsCount/5000000<=0 else iRecordsCount/5000000).write.mode('overwrite').csv()


#--------------option/options--------------------
.option("sep","|")
.option('header', 'true')
.option("compression", "gzip")  -->none/snappy/gzip

.options(sep='|', header='true', quoteAll='true')

#options syntax: 
#.option("sep"="|", "header"="true")


#-------------mode-------------
#.mode('append') #or overwrite

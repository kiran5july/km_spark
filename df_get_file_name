
#---- Scala
import org.apache.spark.sql.functions.input_file_name
df.withColumn("filename", input_file_name)


#---- Pyspark
from pyspark.sql.functions import input_file_name
df = df.withColumn("filename", input_file_name())
=> This gives full file path

#--Use regexp_extract to get just file name (with no path)
from pyspark.sql.functions import regexp_extract
#or: import pyspark.sql.functions as k
regex_str=".*/([^\/]+)$"
df = df.withColumn("filename", k.regexp_extract(k.input_file_name(),regex_str,1))

#--Record count by file name
spark.read.csv('/hdfs/path/').withColumn("filename", F.regexp_extract(F.input_file_name(), regex_file, 1)) \
 .groupBy('filename').count().orderBy('filename').show(50,False)

df_rec_cnts = df.groupBy('filename').agg(k.count('*').alias('count'))
df_rec_cnts.filter(k.col('count')<200000).show(5,False)
df_rec_cnts.orderBy(k.col('count').desc()).show(5,False)



#---- Scala: Get filename manually
var inputHDFSPath ="/hdfs/path"
def funFileName: ((String) => String) = { (s) =>(s.split("/").last)}

import org.apache.spark.sql.functions.udf
val udfFileName = udf(funFileName)

var df_data=spark.read.
 options(Map("delimiter"->"|", "header"->"true", "quoteAll"->"True")).
 csv(inputHDFSPath+ "/*.gz").
 toDF("col1").
 withColumn("file_name", udfFileName(input_file_name()))

